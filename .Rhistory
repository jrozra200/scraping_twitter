install.packages("KernSmooth")
library(KernSmooth)
ls
source('~/Box Documents/Data Science Classes/R Programming/Working Directory/Assignment 2/ProgrammingAssignment2/cachematrix.R')
mat <- matrix(1:4, 2, 2)
x <- makeCacheMatrix(mat)
cacheSolve(x)
cacheSolve(x)
anothermat <- matrix(1:100, 10, 10)
x <- makeCacheMatrix(anothermat)
cacheSolve(x)
setwd("~/Box Documents/Splunk/Twitter")
library(twitteR) ##built in R package that does some of the Twitter API heavy lifting
##Authentication
load("credentials.RData")  ##has my secret keys and shiz
registerTwitterOAuth(twitCred) ##logs me in
##Get the tweets to work with
tweetList <- searchTwitter("comcast email", n = 1000) ##Searches twitter for anything with comcast and email in it
tweetList <- twListToDF(tweetList) ##converts that data we got into a data frame
tweetList
names(tweetList)
library(RTextTools)
library(RTextTools)
library(tm)
matrix <- create_matrix(text,...)
lexicon <- read.csv("./data/emotions.csv.gz",header=FALSE)
counts <- list(anger=length(which(lexicon[,2]=="anger")),disgust=length(which(lexicon[,2]=="disgust")),fear=length(which(lexicon[,2]=="fear")),joy=length(which(lexicon[,2]=="joy")),sadness=length(which(lexicon[,2]=="sadness")),surprise=length(which(lexicon[,2]=="surprise")),total=nrow(lexicon))
documents <- c()
for (i in 1:nrow(matrix)) {
if (verbose) print(paste("DOCUMENT",i))
scores <- list(anger=0,disgust=0,fear=0,joy=0,sadness=0,surprise=0)
doc <- matrix[i,]
words <- findFreqTerms(doc,lowfreq=1)
for (word in words) {
for (key in names(scores)) {
emotions <- lexicon[which(lexicon[,2]==key),]
index <- pmatch(word,emotions[,1],nomatch=0)
if (index > 0) {
entry <- emotions[index,]
category <- as.character(entry[[2]])
count <- counts[[category]]
score <- 1.0
if (algorithm=="bayes") score <- abs(log(score*prior/count))
if (verbose) {
print(paste("WORD:",word,"CAT:",category,"SCORE:",score))
}
scores[[category]] <- scores[[category]]+score
}
}
}
if (algorithm=="bayes") {
for (key in names(scores)) {
count <- counts[[key]]
total <- counts[["total"]]
score <- abs(log(count/total))
scores[[key]] <- scores[[key]]+score
}
} else {
for (key in names(scores)) {
scores[[key]] <- scores[[key]]+0.000001
}
}
best_fit <- names(scores)[which.max(unlist(scores))]
if (best_fit == "disgust" && as.numeric(unlist(scores[2]))-3.09234 < .01) best_fit <- NA
documents <- rbind(documents,c(scores$anger,scores$disgust,scores$fear,scores$joy,scores$sadness,scores$surprise,best_fit))
}
colnames(documents) <- c("ANGER","DISGUST","FEAR","JOY","SADNESS","SURPRISE","BEST_FIT")
head(documents, n = 5)
```
Here you can see that the initial author is using *(I think)* Bayes' theorem to analyze the text. I wanted to show a
quick snipet of how the analysis is being done "under the hood."
For my purposes though, I only care about the emotion outputted and the tweet it is analyzed from.
```{r}
emotion <- documents[, "BEST_FIT"]
head(emotion, n = 5)
library(twitteR) ##built in R package that does some of the Twitter API heavy lifting
##Authentication
load("credentials.RData")  ##has my secret keys and shiz
registerTwitterOAuth(twitCred) ##logs me in
##Get the tweets to work with
tweetList <- searchTwitter("comcast email", n = 1000) ##Searches twitter for anything with comcast and email in it
tweetList <- twListToDF(tweetList) ##converts that data we got into a data frame to make it easier to do further analysis
fixemail <- grep("(fix.*email)", tweetList$text) ##finds the rows that have the phrase "fix ... email" in them
comcastemail <- grep("[Cc]omcast.*email", tweetList$text) ##finds the rows that have the phrase "comcast ... email" in them
noemail <- grep("no email", tweetList$text) ##finds the rows that have the phrase "no email" in them
comcasttweet <- grep("[Cc]omcast", tweetList$screenName) ##finds the rows that originated from a Comcast twitter handle
custserv <- grep("[Cc]ustomer [Ss]ervice.*email|email.*[Cc]ustomer [Ss]ervice", tweetList$text) ##finds the rows related to email and customer service
##combine all of the "good" tweets row numbers that we greped out above and then sorts them and makes sure they are unique
combined <- c(fixemail, comcastemail, noemail, comcasttweet, custserv)
uvals <- unique(combined)
sorted <- sort(uvals)
##pull the row numbers that we want, and with the columns that are important to us (tweet text, time of tweet, source, and username)
paredTweetList <- tweetList[sorted, c(1, 5, 10, 11)]
paredTweetList$statusSource <- sub("<.*\">", "", paredTweetList$statusSource)
paredTweetList$statusSource <- sub("</a>", "", paredTweetList$statusSource)
##name the columns
names(paredTweetList) <- c("Tweet", "Created", "Source", "ScreenName")
head(paredTweetList, n = 10)
library(stringr) ##Does some of the text editing
##Cleaning up the data some more (just the text now...) First grabbing only the text
text <- paredTweetList$Tweet
# remove retweet entities
text <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text)
# remove at people
text <- gsub("@\\w+", "", text)
# remove punctuation
text <- gsub("[[:punct:]]", "", text)
# remove numbers
text <- gsub("[[:digit:]]", "", text)
# remove html links
text <- gsub("http\\w+", "", text)
# define "tolower error handling" function
try.error <- function(x) {
# create missing value
y <- NA
# tryCatch error
try_error <- tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y <- tolower(x)
# result
return(y)
}
# lower case using try.error with sapply
text <- sapply(text, try.error)
# remove NAs in text
text <- text[!is.na(text)]
# remove column names
names(text) <- NULL
library(RTextTools)
library(tm)
matrix <- create_matrix(text,...)
lexicon <- read.csv("./data/emotions.csv.gz",header=FALSE)
counts <- list(anger=length(which(lexicon[,2]=="anger")),disgust=length(which(lexicon[,2]=="disgust")),fear=length(which(lexicon[,2]=="fear")),joy=length(which(lexicon[,2]=="joy")),sadness=length(which(lexicon[,2]=="sadness")),surprise=length(which(lexicon[,2]=="surprise")),total=nrow(lexicon))
documents <- c()
for (i in 1:nrow(matrix)) {
if (verbose) print(paste("DOCUMENT",i))
scores <- list(anger=0,disgust=0,fear=0,joy=0,sadness=0,surprise=0)
doc <- matrix[i,]
words <- findFreqTerms(doc,lowfreq=1)
for (word in words) {
for (key in names(scores)) {
emotions <- lexicon[which(lexicon[,2]==key),]
index <- pmatch(word,emotions[,1],nomatch=0)
if (index > 0) {
entry <- emotions[index,]
category <- as.character(entry[[2]])
count <- counts[[category]]
score <- 1.0
if (algorithm=="bayes") score <- abs(log(score*prior/count))
if (verbose) {
print(paste("WORD:",word,"CAT:",category,"SCORE:",score))
}
scores[[category]] <- scores[[category]]+score
}
}
}
if (algorithm=="bayes") {
for (key in names(scores)) {
count <- counts[[key]]
total <- counts[["total"]]
score <- abs(log(count/total))
scores[[key]] <- scores[[key]]+score
}
} else {
for (key in names(scores)) {
scores[[key]] <- scores[[key]]+0.000001
}
}
best_fit <- names(scores)[which.max(unlist(scores))]
if (best_fit == "disgust" && as.numeric(unlist(scores[2]))-3.09234 < .01) best_fit <- NA
documents <- rbind(documents,c(scores$anger,scores$disgust,scores$fear,scores$joy,scores$sadness,scores$surprise,best_fit))
}
colnames(documents) <- c("ANGER","DISGUST","FEAR","JOY","SADNESS","SURPRISE","BEST_FIT")
head(documents, n = 5)
library(RTextTools)
library(tm)
matrix <- create_matrix(text)
lexicon <- read.csv("./data/emotions.csv.gz",header=FALSE)
counts <- list(anger=length(which(lexicon[,2]=="anger")),disgust=length(which(lexicon[,2]=="disgust")),fear=length(which(lexicon[,2]=="fear")),joy=length(which(lexicon[,2]=="joy")),sadness=length(which(lexicon[,2]=="sadness")),surprise=length(which(lexicon[,2]=="surprise")),total=nrow(lexicon))
documents <- c()
for (i in 1:nrow(matrix)) {
if (verbose) print(paste("DOCUMENT",i))
scores <- list(anger=0,disgust=0,fear=0,joy=0,sadness=0,surprise=0)
doc <- matrix[i,]
words <- findFreqTerms(doc,lowfreq=1)
for (word in words) {
for (key in names(scores)) {
emotions <- lexicon[which(lexicon[,2]==key),]
index <- pmatch(word,emotions[,1],nomatch=0)
if (index > 0) {
entry <- emotions[index,]
category <- as.character(entry[[2]])
count <- counts[[category]]
score <- 1.0
if (algorithm=="bayes") score <- abs(log(score*prior/count))
if (verbose) {
print(paste("WORD:",word,"CAT:",category,"SCORE:",score))
}
scores[[category]] <- scores[[category]]+score
}
}
}
if (algorithm=="bayes") {
for (key in names(scores)) {
count <- counts[[key]]
total <- counts[["total"]]
score <- abs(log(count/total))
scores[[key]] <- scores[[key]]+score
}
} else {
for (key in names(scores)) {
scores[[key]] <- scores[[key]]+0.000001
}
}
best_fit <- names(scores)[which.max(unlist(scores))]
if (best_fit == "disgust" && as.numeric(unlist(scores[2]))-3.09234 < .01) best_fit <- NA
documents <- rbind(documents,c(scores$anger,scores$disgust,scores$fear,scores$joy,scores$sadness,scores$surprise,best_fit))
}
colnames(documents) <- c("ANGER","DISGUST","FEAR","JOY","SADNESS","SURPRISE","BEST_FIT")
head(documents, n = 5)
algorithm <- "bayes"
prior <- 1.0
verbose <- FALSE
matrix <- create_matrix(text)
lexicon <- read.csv("./data/emotions.csv.gz",header=FALSE)
counts <- list(anger=length(which(lexicon[,2]=="anger")),disgust=length(which(lexicon[,2]=="disgust")),fear=length(which(lexicon[,2]=="fear")),joy=length(which(lexicon[,2]=="joy")),sadness=length(which(lexicon[,2]=="sadness")),surprise=length(which(lexicon[,2]=="surprise")),total=nrow(lexicon))
documents <- c()
for (i in 1:nrow(matrix)) {
if (verbose) print(paste("DOCUMENT",i))
scores <- list(anger=0,disgust=0,fear=0,joy=0,sadness=0,surprise=0)
doc <- matrix[i,]
words <- findFreqTerms(doc,lowfreq=1)
for (word in words) {
for (key in names(scores)) {
emotions <- lexicon[which(lexicon[,2]==key),]
index <- pmatch(word,emotions[,1],nomatch=0)
if (index > 0) {
entry <- emotions[index,]
category <- as.character(entry[[2]])
count <- counts[[category]]
score <- 1.0
if (algorithm=="bayes") score <- abs(log(score*prior/count))
if (verbose) {
print(paste("WORD:",word,"CAT:",category,"SCORE:",score))
}
scores[[category]] <- scores[[category]]+score
}
}
}
if (algorithm=="bayes") {
for (key in names(scores)) {
count <- counts[[key]]
total <- counts[["total"]]
score <- abs(log(count/total))
scores[[key]] <- scores[[key]]+score
}
} else {
for (key in names(scores)) {
scores[[key]] <- scores[[key]]+0.000001
}
}
best_fit <- names(scores)[which.max(unlist(scores))]
if (best_fit == "disgust" && as.numeric(unlist(scores[2]))-3.09234 < .01) best_fit <- NA
documents <- rbind(documents,c(scores$anger,scores$disgust,scores$fear,scores$joy,scores$sadness,scores$surprise,best_fit))
}
colnames(documents) <- c("ANGER","DISGUST","FEAR","JOY","SADNESS","SURPRISE","BEST_FIT")
head(documents, n = 5)
emotion <- documents[, "BEST_FIT"]
head(emotion, n = 5)
algorithm <- "bayes"
pstrong <- 0.5
pweak <- 1.0
prior <- 1.0
verbose <- FALSE
matrix <- create_matrix(text)
lexicon <- read.csv("./data/subjectivity.csv.gz",header=FALSE)
counts <- list(positive=length(which(lexicon[,3]=="positive")),negative=length(which(lexicon[,3]=="negative")),total=nrow(lexicon))
documents <- c()
for (i in 1:nrow(matrix)) {
if (verbose) print(paste("DOCUMENT",i))
scores <- list(positive=0,negative=0)
doc <- matrix[i,]
words <- findFreqTerms(doc,lowfreq=1)
for (word in words) {
index <- pmatch(word,lexicon[,1],nomatch=0)
if (index > 0) {
entry <- lexicon[index,]
polarity <- as.character(entry[[2]])
category <- as.character(entry[[3]])
count <- counts[[category]]
score <- pweak
if (polarity == "strongsubj") score <- pstrong
if (algorithm=="bayes") score <- abs(log(score*prior/count))
if (verbose) {
print(paste("WORD:",word,"CAT:",category,"POL:",polarity,"SCORE:",score))
}
scores[[category]] <- scores[[category]]+score
}
}
if (algorithm=="bayes") {
for (key in names(scores)) {
count <- counts[[key]]
total <- counts[["total"]]
score <- abs(log(count/total))
scores[[key]] <- scores[[key]]+score
}
} else {
for (key in names(scores)) {
scores[[key]] <- scores[[key]]+0.000001
}
}
best_fit <- names(scores)[which.max(unlist(scores))]
ratio <- as.integer(abs(scores$positive/scores$negative))
if (ratio==1) best_fit <- "neutral"
documents <- rbind(documents,c(scores$positive,scores$negative,abs(scores$positive/scores$negative),best_fit))
if (verbose) {
print(paste("POS:",scores$positive,"NEG:",scores$negative,"RATIO:",abs(scores$positive/scores$negative)))
cat("\n")
}
}
colnames(documents) <- c("POS","NEG","POS/NEG","BEST_FIT")
head(documents, n = 5)
polarity <- documents[, "BEST_FIT"]
totalFinalOutput <- cbind(paredTweetList$Tweet, emotion, polarity)
colnames(totalFinalOutput) <- c("tweet", "emotion", "polarity")
head(totalFinalOutput)
library(plotrix) ##Alows for pie charts
##segment the matrix by sentiment
postweets <- results[totalFinalOutput$polarity == "positive", ]
neuttweets <- results[totalFinalOutput$polarity == "neutral", ]
negtweets <- results[totalFinalOutput$polarity == "negative", ]
##get a value for the number of rows in each matrix
total <- dim(results)[1]
posnum <- dim(postweets)[1]
negnum <- dim(negtweets)[1]
neutnum <- dim(neuttweets)[1]
##calculate the percentage for each sentiment value
posperc <- posnum/total
negperc <- negnum/total
neutperc <- neutnum/total
##create the pie chart slices, labels, and add the percentage
slices <- c(posnum, negnum, neutnum)
lbls <- c("Positive", "Negative", "Neutral")
pct <- round(slices/sum(slices)*100)
lbls <- paste(lbls, pct)
lbls <- paste(lbls, "%", sep = "")
pie3D(slices, labels=lbls, explode=0.1, main="Overall Sentiment of Comcast Email on Twitter")
postweets <- totalFinalOutput[totalFinalOutput$polarity == "positive", ]
neuttweets <- totalFinalOutput[totalFinalOutput$polarity == "neutral", ]
negtweets <- totalFinalOutput[totalFinalOutput$polarity == "negative", ]
##get a value for the number of rows in each matrix
total <- dim(totalFinalOutput)[1]
posnum <- dim(postweets)[1]
negnum <- dim(negtweets)[1]
neutnum <- dim(neuttweets)[1]
##calculate the percentage for each sentiment value
posperc <- posnum/total
negperc <- negnum/total
neutperc <- neutnum/total
##create the pie chart slices, labels, and add the percentage
slices <- c(posnum, negnum, neutnum)
lbls <- c("Positive", "Negative", "Neutral")
pct <- round(slices/sum(slices)*100)
lbls <- paste(lbls, pct)
lbls <- paste(lbls, "%", sep = "")
pie3D(slices, labels=lbls, explode=0.1, main="Overall Sentiment of Comcast Email on Twitter")
class(totalFinalOutput)
totalFinalOutput <- as.data.frame(totalFinalOutput)
postweets <- totalFinalOutput[totalFinalOutput$polarity == "positive", ]
neuttweets <- totalFinalOutput[totalFinalOutput$polarity == "neutral", ]
negtweets <- totalFinalOutput[totalFinalOutput$polarity == "negative", ]
##get a value for the number of rows in each matrix
total <- dim(totalFinalOutput)[1]
posnum <- dim(postweets)[1]
negnum <- dim(negtweets)[1]
neutnum <- dim(neuttweets)[1]
##calculate the percentage for each sentiment value
posperc <- posnum/total
negperc <- negnum/total
neutperc <- neutnum/total
##create the pie chart slices, labels, and add the percentage
slices <- c(posnum, negnum, neutnum)
lbls <- c("Positive", "Negative", "Neutral")
pct <- round(slices/sum(slices)*100)
lbls <- paste(lbls, pct)
lbls <- paste(lbls, "%", sep = "")
pie3D(slices, labels=lbls, explode=0.1, main="Overall Sentiment of Comcast Email on Twitter")
head(totalFinalOutput)
class(totalFinalOutput)
