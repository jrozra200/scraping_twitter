complete <- function(directory, id = 1:332) {#
        setwd(directory) #sets the working directory as specified#
        counter <- 1 #initializing a counter that will be used to assign the row of the data frame#
        nobs <- data.frame() #and here is the data frame we will use - initializing it#
        for(i in id){#
                #creates the filename from the number in the loop depending on the size of the number#
                filename <- if(i < 10) {#
                        paste("00", i, ".csv", sep = "")#
                } else if (i >= 10 && i < 100) {#
                        paste("0", i, ".csv", sep = "")#
                } else {#
                        paste(i, ".csv", sep = "")#
                }#
                data <- read.csv(filename) #assign the file's data to the variable titled data#
                iscomplete <- complete.cases(data) #determine which of the cases are complete#
                gooddata <- data[iscomplete, ] #assign only the good cases to a new variable#
                nobs[counter, 1] <- i #collumn one is the ID of the file we are in#
                nobs[counter, 2] <- nrow(gooddata) #collumn two is the number of complete cases#
                counter <- counter + 1 #adding one to go to the next row on the next go round#
        }#
        names(nobs) <- c("id", "nobs") #assigns names to our data frame#
        nobs #returns the data frame with the sums for each file's complete cases#
}
x <- corr("users/jrozra200/Box Documents/Data Science Classes/R Programming/Working Directory")
x <- corr("users/jrozra200/Box Documents/Data Science Classes/R Programming/Working Directory/specdata")
corr <- function(directory, threshold = 0) {#
        #setwd(directory)#
        data <- complete(directory) #gets the number of complete data from the complete function for each file#
        threshIDs <- vector("numeric") #creates an empty numeric vector to contain those IDs that meet the threshold#
        for(i in data$id) { #goes through the data obtained in data and identifies the IDs that meet the threshold#
                if(data$nobs[i] <= threshold) {#
                        next#
                } else {#
                        threshIDs <- append(threshIDs, data$id[i])#
                }#
        }#
        cors <- vector("numeric") #creates an empty vector which will contain the cor values#
        for(i in threshIDs){#
                #creates the filename from the number in the loop depending on the size of the number#
                filename <- if(i < 10) {#
                        paste("00", i, ".csv", sep = "")#
                } else if (i >= 10 && i < 100) {#
                        paste("0", i, ".csv", sep = "")#
                } else {#
                        paste(i, ".csv", sep = "")#
                }#
                rawdata <- read.csv(filename) #assign the file's data to the variable titled data#
                iscomplete <- complete.cases(rawdata) #determine which of the cases are complete#
                gooddata <- rawdata[iscomplete, ] #assign only the good cases to a new variable#
                sulfate <- gooddata$sulfate #separates out just the good sulfate data#
                nitrate <- gooddata$nitrate #separates out just the good nitrate data#
                cors <- append(cors, cor(sulfate, nitrate)) #adds the cor values for each file to the vector#
        }#
        cors #returns all the cor values#
}
x <- corr("users/jrozra200/Box Documents/Data Science Classes/R Programming/Working Directory/specdata")
cube <- function(x, n) {#
        x^3#
}
cube(3)
x <- 1:10#
if(x > 5) {#
        x <- 0#
}
f <- function(x) {#
        g <- function(y) {#
                y + z#
        }#
        z <- 4#
        x + g(x)#
}
z <- 10
f(3)
x <- 5#
y <- if(x < 3) {#
        NA#
} else {#
        10#
}
y
cbind(y, 3L)
list <- random(100)
list <- rand()
list <- sample(1:1000000, 10000, replace = TRUE)
summary(list)
lapply
print
str
traceback
debug
x <- matrix(1:4, 2, 2)
x
y <- solve(x)
y
x <- matrix(1:400, 20, 20)
x
y <- solve(x)
y
makeVector <- function(x = numeric()) {#
        m <- NULL#
        set <- function(y) {#
                x <<- y#
                m <<- NULL#
        }#
        get <- function() x#
        setmean <- function(mean) m <<- mean#
        getmean <- function() m#
        list(set = set, get = get,#
             setmean = setmean,#
             getmean = getmean)#
}
cachemean <- function(x, ...) {#
        m <- x$getmean()#
        if(!is.null(m)) {#
                message("getting cached data")#
                return(m)#
        }#
        data <- x$get()#
        m <- mean(data, ...)#
        x$setmean(m)#
        m#
}
x <- vector(1:20)
x <- 1:20
cachemean(x)
x <- makeVector()
cachemean(x)
x <- makeVector(1:20)
cachemean(x)
library(datasets)#
data(iris)
?iris
mean(iris$Sepal.Length)
iris$Sepal.Length
head(iris)
colmeans(iris)
colMeans(iris)
apply(iris[, 1:4], 2, mean)
library(datasets)#
data(mtcars)
head(mtcars)
str(mtcars)
tapply(mtcars$mpg, mtcars$cyl, mean)
tapply(mtcars$cyl, mtcars$mpg, mean)
tapply(mtcars$mpg, mtcars$cyl, mean)
with(mtcars, tappy(mpg, cyl, mean))
?with
mean(mtcars$mpg, mtcars$cyl)
lapply(mtcars, mean)
x <- tapply(mtcars$mpg, mtcars$cyl, mean)
x$8 - x$4
x[8] - x[4]
x
x["8"]
x["8"] - x["4"]
debug(ls)
ls
with(mtcars, tapply(mpg, cyl, mean))
x <- as.numeric(mean(iris$Sepal.Length))
x
tapply(mtcars$mpg, mtcars$cyl, mean)
x <- tapply(mtcars$mpg, mtcars$cyl, mean)
x["4"] - x["8"]
colMeans(iris$Sepal.Length)
colMeans(iris, iris$Sepal.Length)
iris
head(iris)
colMeans(iris)
colMeans(iris[, 1:4])
x <- split(iris, iris$Species)
x
mean(x$virginica["Sepal.Length"])
y <- x$virginica
head(y)
mean(y$Sepal.Length)
tapply(mtcars$mpg, mtcars$cyl, mean)
26.66364 - 15.1
head(mtcars)
tapply(mtcars$hp, mtcars$cyl, mean)
209.21429 - 82.63636
tapply(iris$Sepal.Length, iris$Species, mean)
library(swirl)
install_from_swirl("Getting and Cleaning Data")
swirl()
mydf <- read.csv(path2csv, stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageVersion("dplyr")
cran <- tbl_df(mydf)
rm("mydf")
cran
?select
select(cran, ip_id, package, country)
5:20
select(cran, r_arch:country)
select(cran, country:r_arch)
cran
select(cran, -time)
-5:20
-(5:20)
select(cran, -(X:size))
filter(cran, package == "swirl")
filter(cran, r_version == "3.1.1", country == "US")
?Comparison
filter(cran, r_version <= "3.0.2", country == "IN")
filter(cran, country == "US | country == "IN")
filter(cran, country == "US" | country == "IN")
filter(cran, size > 100500, r_os == "linux-gnu")
is.na(c(3, 5, NA, 10))
!is.na(c(3, 5, NA, 10))
filter(cran, !(is.na(r_version)))
filter(cran, !is.na(r_version))
select(cran, size:ip_id)
cran2 <- select(cran, size:ip_id)
arrange(cran2, ip_id)
arrange(cran2, desc(ip_id))
arrange(cran2, package, ip_id)
arrange(cran2, country, desc(r_version) ip_id)
arrange(cran2, country, desc(r_version), ip_id)
cran3 <- select(cran, ip_id, package, size)
cran3
mutate(cran3, size_mb = size / 2^20)
mutate(cran3, size_mb = size / 2^20, size_gb = size / 2^10)
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10, correct_size = size - 1000)
mutate(cran3, correct_size = size - 1000)
mutate(cran3, correct_size = size + 1000)
summarize(cran, avg_bytes = mean(size))
swirl()
library(swirl)
swirl()
library(dplyer)
library(dplyr)
cran <- tbl_df(mydf)
rm("mydf")
cran
?group_by
by_package <- group_by(cran, package)
by_package
summarize(by_package, mean(size))
?n
submit()
pack_sum
quantile(pack_sum$count, probs = 0.99)
top_counts <- filter(pack_sum, count > 679)
top_counts
head(top_counts, 20)
arrange(top_counts, count)
arrange(top_counts, desc(count))
quantile(pack_sum$unique, probs = 0.99)
top_unique <- filter(pack_sum, unique > 465)
top_unique
arrange(pack_sum, desc(unique))
arrange(top_unique, desc(unique))
submit()
library(swirl)
swirl()
submit()
?spread
reset()
submit()
skip()
extract_numeric("class5")
submit()
?mutate
submit()
students4
submit()
passed
failed
passed <- mutate(passed, "passed")
passed <- passed %>% mutate(status = "passed")
failed <- failed %>% mutate(status = "failed")
rbind_list(passed, failed)
sat
?select
?separate
submit()
?group_by
submit()
install.packages("RMySQL")
library(RMySQL)
install.packages("RMySQL")
source("http://bioconductor.org/bioclite.R")
source("http://bioconductor.org/biocLite.R")
biocLite("rhdf5")
library(rdhf5)
library(rhdf5)
InsectSprays
with(InsectSprays, tappy(count, spray, mean))
with(InsectSprays, tapply(count, spray, mean))
with(InsectSprays, by(count, spray, mean))
aggregate(count ~ spray, InsectSprays, mean)
with(InsectSprays, split(count, spray))
?text
?postscript
install.packages("twittR")
library(lattice)
x <- rnorm(100)
y <- rnorm(100)
xyplot(x,y)
xyplot(x, y)
xyplot(y ~ x| f * g)
?xyplot
library(nlme)#
library(lattice)#
xyplot(weight ~ Time | Diet, BodyWeight)
x <- xyplot(weight ~ Time | Diet, BodyWeight)
class(x)
?lines
?points
?llines
library(lattice)#
library(datasets)#
data(airquality)#
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
p
?trellis.par.set
?print.trellis
?splom
?par
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
library(ggplot2)
install.packages("ggplot2")
library(ggplot2)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
qplot(Wind, Ozone, data = airquality)
airquality = transform(airquality, Month = factor(Month))#
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
?geom
g <- ggplot(movies, aes(votes, rating))#
print(g)
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) + geom_smooth()
library(Rfacebook)
library(devtools)
load("FBcredentials.RData")
library(twitteR) ##built in R package that does some of the Twitter API heavy lifting#
        library(stringr) ##Does some of the text editing#
        library(wordcloud) ##Lets me do word clouds#
        library(RColorBrewer) ##Allows for many different color pallets#
        library(plotrix) ##Alows for pie charts#
        ##Authentication#
        load("credentials.RData")  ##has my secret keys and shiz#
        registerTwitterOAuth(twitCred) ##logs me in#
        ##Get the tweets to work with#
        tweetList <- searchTwitter("comcast email", n = 1000) ##Searches twitter for anything with comcast and email in it#
        tweetList <- twListToDF(tweetList) ##converts that data we got into a data frame#
        fixemail <- grep("(fix.*email)", tweetList$text) ##finds the rows that have the phrase "fix ... email" in them#
        comcastemail <- grep("[Cc]omcast.*email", tweetList$text) ##finds the rows that have the phrase "comcast ... email" in them#
        noemail <- grep("no email", tweetList$text) ##finds the rows that have the phrase "no email" in them#
        comcasttweet <- grep("[Cc]omcast", tweetList$screenName) ##finds the rows that originated from a Comcast twitter handle#
        custserv <- grep("[Cc]ustomer [Ss]ervice.*email|email.*[Cc]ustomer [Ss]ervice", tweetList$text) ##finds the rows related to email and customer service#
        ##combine all of the "good" tweets row numbers that we greped out above and then sorts them and makes sure they are unique#
        combined <- c(fixemail, comcastemail, noemail, comcasttweet, custserv)#
        uvals <- unique(combined)#
        sorted <- sort(uvals)#
        ##pull the row numbers that we want, and with the columns that are important to us (tweet text, time of tweet, source, and username)#
        paredTweetList <- tweetList[sorted, c(1, 5, 10, 11)] #
        ##make the device source look nicer#
        paredTweetList$statusSource <- sub("<.*\">", "", paredTweetList$statusSource) #
        paredTweetList$statusSource <- sub("</a>", "", paredTweetList$statusSource)#
        ##name the columns#
        names(paredTweetList) <- c("Tweet", "Created", "Source", "ScreenName")#
        ##write the output to a csv... commenting this out for now - will bring it back if we pull this into R for visualization#
        ## write.table(paredTweetList, "testfile.csv", sep = ",", row.names = FALSE, qmethod = "double")#
        ##NOW some sentiment analysis#
        ##Cleaning up the data some more (just the text now...) First grabbing only the text#
        text <- paredTweetList$Tweet#
        # remove retweet entities#
        text <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text)#
        # remove at people#
        text <- gsub("@\\w+", "", text)#
        # remove punctuation#
        text <- gsub("[[:punct:]]", "", text)#
        # remove numbers#
        text <- gsub("[[:digit:]]", "", text)#
        # remove html links#
        text <- gsub("http\\w+", "", text)#
        # remove unnecessary spaces - NEED TO RELOOK AT THIS - didn't seem to work right#
        #text <- gsub("[ \t]{2,}", "", text)#
        #text <- gsub("^\\s+|\\s+$", "", text)#
        # define "tolower error handling" function #
        try.error <- function(x)#
        {#
                # create missing value#
                y <- NA#
                # tryCatch error#
                try_error <- tryCatch(tolower(x), error=function(e) e)#
                # if not an error#
                if (!inherits(try_error, "error"))#
                        y <- tolower(x)#
                # result#
                return(y)#
        }#
        # lower case using try.error with sapply #
        text <- sapply(text, try.error)#
        # remove NAs in text#
        text <- text[!is.na(text)]#
        names(text) <- NULL#
        #load up word polarity list and format it#
        afinn_list <- read.delim("AFINN-111.txt", header=FALSE, stringsAsFactors=FALSE)#
        names(afinn_list) <- c('word', 'score')#
        afinn_list$word <- tolower(afinn_list$word)#
        ##segment the terms for evaluation by severity#
        vNegTerms <- afinn_list$word[afinn_list$score==-5 | afinn_list$score==-4]#
        negTerms <- afinn_list$word[afinn_list$score==-3 | afinn_list$score==-2 | afinn_list$score==-1]#
        posTerms <- afinn_list$word[afinn_list$score==3 | afinn_list$score==2 | afinn_list$score==1]#
        vPosTerms <- afinn_list$word[afinn_list$score==5 | afinn_list$score==4]#
        ##create a matrix to store the scores and sentiment#
        final_scores <- matrix('', 0, 6)#
        ##go through each sentence in the matrix#
        for(i in text) {#
                ##pull out each word#
                wordList <- str_split(i, '\\s+')#
                words <- unlist(wordList)#
                ##match every class of word#
                vPosMatches <- match(words, vPosTerms)#
                posMatches <- match(words, posTerms)#
                vNegMatches <- match(words, vNegTerms)#
                negMatches <- match(words, negTerms)#
                #sum up number of words in each category#
                vPosMatches <- sum(!is.na(vPosMatches))#
                posMatches <- sum(!is.na(posMatches))#
                vNegMatches <- sum(!is.na(vNegMatches))#
                negMatches <- sum(!is.na(negMatches))#
                score <- c(vNegMatches, negMatches, posMatches, vPosMatches)   #
                #add row to scores table#
                newrow <- c(i, score)#
                final_scores <- rbind(final_scores, newrow)#
        }#
        ##rename the scored data and change it to a data frame#
        results <- as.data.frame(final_scores)#
        ##name the columns#
        colnames(results) <- c('sentence', 'vNeg', 'neg', 'pos', 'vPos', 'sentiment')#
        ##coerce the values to numerics (for some reason each value is incremented by 1, so adjusting for this as well)#
        results[, 2] <- as.numeric(results[, 2]); results[, 2] <- results[, 2] - 1#
        results[, 3] <- as.numeric(results[, 3]); results[, 3] <- results[, 3] - 1#
        results[, 4] <- as.numeric(results[, 4]); results[, 4] <- results[, 4] - 1#
        results[, 5] <- as.numeric(results[, 5]); results[, 5] <- results[, 5] - 1#
        results[, 6] <- NA ##empting out the sentiment column, to which we will be adding our answer below#
        ##counter for the while statment - we will go through each row in the matrix#
        counter <- 1#
        while(counter <= nrow(results)) {#
                ##calculate the overall sentiment of the tweet#
                sentcount <- (results[counter, 2] * -2) + (results[counter, 3] * -1) + results[counter, 4] + (results[counter, 5] * 2)#
                ##assign the overall sentiment to the sentiment column#
                results[counter, 6] <- if(sentcount < -1) {#
                        "negative"#
                } else if(sentcount > 1) {#
                        "positive"#
                } else {#
                        "neutral"#
                }#
                counter <- counter + 1#
        }
library(twitteR) ##built in R package that does some of the Twitter API heavy lifting#
        library(stringr) ##Does some of the text editing#
        library(wordcloud) ##Lets me do word clouds#
        library(RColorBrewer) ##Allows for many different color pallets#
        library(plotrix) ##Alows for pie charts#
        ##Authentication#
        load("credentials.RData")  ##has my secret keys and shiz#
        registerTwitterOAuth(twitCred) ##logs me in#
        ##Get the tweets to work with#
        tweetList <- searchTwitter("comcast email", n = 1000) ##Searches twitter for anything with comcast and email in it#
        tweetList <- twListToDF(tweetList) ##converts that data we got into a data frame#
        fixemail <- grep("(fix.*email)", tweetList$text) ##finds the rows that have the phrase "fix ... email" in them#
        comcastemail <- grep("[Cc]omcast.*email", tweetList$text) ##finds the rows that have the phrase "comcast ... email" in them#
        noemail <- grep("no email", tweetList$text) ##finds the rows that have the phrase "no email" in them#
        comcasttweet <- grep("[Cc]omcast", tweetList$screenName) ##finds the rows that originated from a Comcast twitter handle#
        custserv <- grep("[Cc]ustomer [Ss]ervice.*email|email.*[Cc]ustomer [Ss]ervice", tweetList$text) ##finds the rows related to email and customer service#
        ##combine all of the "good" tweets row numbers that we greped out above and then sorts them and makes sure they are unique#
        combined <- c(fixemail, comcastemail, noemail, comcasttweet, custserv)#
        uvals <- unique(combined)#
        sorted <- sort(uvals)#
        ##pull the row numbers that we want, and with the columns that are important to us (tweet text, time of tweet, source, and username)#
        paredTweetList <- tweetList[sorted, c(1, 5, 10, 11)] #
        ##make the device source look nicer#
        paredTweetList$statusSource <- sub("<.*\">", "", paredTweetList$statusSource) #
        paredTweetList$statusSource <- sub("</a>", "", paredTweetList$statusSource)#
        ##name the columns#
        names(paredTweetList) <- c("Tweet", "Created", "Source", "ScreenName")#
        ##write the output to a csv... commenting this out for now - will bring it back if we pull this into R for visualization#
        ## write.table(paredTweetList, "testfile.csv", sep = ",", row.names = FALSE, qmethod = "double")#
        ##NOW some sentiment analysis#
        ##Cleaning up the data some more (just the text now...) First grabbing only the text#
        text <- paredTweetList$Tweet#
        # remove retweet entities#
        text <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text)#
        # remove at people#
        text <- gsub("@\\w+", "", text)#
        # remove punctuation#
        text <- gsub("[[:punct:]]", "", text)#
        # remove numbers#
        text <- gsub("[[:digit:]]", "", text)#
        # remove html links#
        text <- gsub("http\\w+", "", text)#
        # remove unnecessary spaces - NEED TO RELOOK AT THIS - didn't seem to work right#
        #text <- gsub("[ \t]{2,}", "", text)#
        #text <- gsub("^\\s+|\\s+$", "", text)#
        # define "tolower error handling" function #
        try.error <- function(x)#
        {#
                # create missing value#
                y <- NA#
                # tryCatch error#
                try_error <- tryCatch(tolower(x), error=function(e) e)#
                # if not an error#
                if (!inherits(try_error, "error"))#
                        y <- tolower(x)#
                # result#
                return(y)#
        }#
        # lower case using try.error with sapply #
        text <- sapply(text, try.error)#
        # remove NAs in text#
        text <- text[!is.na(text)]#
        names(text) <- NULL#
        #load up word polarity list and format it#
        afinn_list <- read.delim("AFINN-111.txt", header=FALSE, stringsAsFactors=FALSE)#
        names(afinn_list) <- c('word', 'score')#
        afinn_list$word <- tolower(afinn_list$word)#
        ##segment the terms for evaluation by severity#
        vNegTerms <- afinn_list$word[afinn_list$score==-5 | afinn_list$score==-4]#
        negTerms <- afinn_list$word[afinn_list$score==-3 | afinn_list$score==-2 | afinn_list$score==-1]#
        posTerms <- afinn_list$word[afinn_list$score==3 | afinn_list$score==2 | afinn_list$score==1]#
        vPosTerms <- afinn_list$word[afinn_list$score==5 | afinn_list$score==4]#
        ##create a matrix to store the scores and sentiment#
        final_scores <- matrix('', 0, 6)#
        ##go through each sentence in the matrix#
        for(i in text) {#
                ##pull out each word#
                wordList <- str_split(i, '\\s+')#
                words <- unlist(wordList)#
                ##match every class of word#
                vPosMatches <- match(words, vPosTerms)#
                posMatches <- match(words, posTerms)#
                vNegMatches <- match(words, vNegTerms)#
                negMatches <- match(words, negTerms)#
                #sum up number of words in each category#
                vPosMatches <- sum(!is.na(vPosMatches))#
                posMatches <- sum(!is.na(posMatches))#
                vNegMatches <- sum(!is.na(vNegMatches))#
                negMatches <- sum(!is.na(negMatches))#
                score <- c(vNegMatches, negMatches, posMatches, vPosMatches)   #
                #add row to scores table#
                newrow <- c(i, score)#
                final_scores <- rbind(final_scores, newrow)#
        }#
        ##rename the scored data and change it to a data frame#
        results <- as.data.frame(final_scores)#
        ##name the columns#
        colnames(results) <- c('sentence', 'vNeg', 'neg', 'pos', 'vPos', 'sentiment')#
        ##coerce the values to numerics (for some reason each value is incremented by 1, so adjusting for this as well)#
        results[, 2] <- as.numeric(results[, 2]); results[, 2] <- results[, 2] - 1#
        results[, 3] <- as.numeric(results[, 3]); results[, 3] <- results[, 3] - 1#
        results[, 4] <- as.numeric(results[, 4]); results[, 4] <- results[, 4] - 1#
        results[, 5] <- as.numeric(results[, 5]); results[, 5] <- results[, 5] - 1#
        results[, 6] <- NA ##empting out the sentiment column, to which we will be adding our answer below#
        ##counter for the while statment - we will go through each row in the matrix#
        counter <- 1#
        while(counter <= nrow(results)) {#
                ##calculate the overall sentiment of the tweet#
                sentcount <- (results[counter, 2] * -2) + (results[counter, 3] * -1) + results[counter, 4] + (results[counter, 5] * 2)#
                ##assign the overall sentiment to the sentiment column#
                results[counter, 6] <- if(sentcount < -1) {#
                        "negative"#
                } else if(sentcount > 1) {#
                        "positive"#
                } else {#
                        "neutral"#
                }#
                counter <- counter + 1#
        }
results
final_scores
tweetList <- searchTwitter("comcast email", n = 1000) ##Searches twitter for anything with comcast and email in it#
        tweetList <- twListToDF(tweetList) ##converts that data we got into a data frame#
        fixemail <- grep("(fix.*email)", tweetList$text) ##finds the rows that have the phrase "fix ... email" in them#
        comcastemail <- grep("[Cc]omcast.*email", tweetList$text) ##finds the rows that have the phrase "comcast ... email" in them#
        noemail <- grep("no email", tweetList$text) ##finds the rows that have the phrase "no email" in them#
        comcasttweet <- grep("[Cc]omcast", tweetList$screenName) ##finds the rows that originated from a Comcast twitter handle#
        custserv <- grep("[Cc]ustomer [Ss]ervice.*email|email.*[Cc]ustomer [Ss]ervice", tweetList$text) ##finds the rows related to email and customer service#
        ##combine all of the "good" tweets row numbers that we greped out above and then sorts them and makes sure they are unique#
        combined <- c(fixemail, comcastemail, noemail, comcasttweet, custserv)#
        uvals <- unique(combined)#
        sorted <- sort(uvals)#
        ##pull the row numbers that we want, and with the columns that are important to us (tweet text, time of tweet, source, and username)#
        paredTweetList <- tweetList[sorted, c(1, 5, 10, 11)] #
        ##make the device source look nicer#
        paredTweetList$statusSource <- sub("<.*\">", "", paredTweetList$statusSource) #
        paredTweetList$statusSource <- sub("</a>", "", paredTweetList$statusSource)#
        ##name the columns#
        names(paredTweetList) <- c("Tweet", "Created", "Source", "ScreenName")#
        ##write the output to a csv... commenting this out for now - will bring it back if we pull this into R for visualization#
        ## write.table(paredTweetList, "testfile.csv", sep = ",", row.names = FALSE, qmethod = "double")#
        ##NOW some sentiment analysis#
        ##Cleaning up the data some more (just the text now...) First grabbing only the text#
        text <- paredTweetList$Tweet#
        # remove retweet entities#
        text <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text)#
        # remove at people#
        text <- gsub("@\\w+", "", text)#
        # remove punctuation#
        text <- gsub("[[:punct:]]", "", text)#
        # remove numbers#
        text <- gsub("[[:digit:]]", "", text)#
        # remove html links#
        text <- gsub("http\\w+", "", text)#
        # remove unnecessary spaces - NEED TO RELOOK AT THIS - didn't seem to work right#
        #text <- gsub("[ \t]{2,}", "", text)#
        #text <- gsub("^\\s+|\\s+$", "", text)#
        # define "tolower error handling" function #
        try.error <- function(x)#
        {#
                # create missing value#
                y <- NA#
                # tryCatch error#
                try_error <- tryCatch(tolower(x), error=function(e) e)#
                # if not an error#
                if (!inherits(try_error, "error"))#
                        y <- tolower(x)#
                # result#
                return(y)#
        }#
        # lower case using try.error with sapply #
        text <- sapply(text, try.error)#
        # remove NAs in text#
        text <- text[!is.na(text)]#
        names(text) <- NULL#
        #load up word polarity list and format it#
        afinn_list <- read.delim("AFINN-111.txt", header=FALSE, stringsAsFactors=FALSE)#
        names(afinn_list) <- c('word', 'score')#
        afinn_list$word <- tolower(afinn_list$word)#
        ##segment the terms for evaluation by severity#
        vNegTerms <- afinn_list$word[afinn_list$score==-5 | afinn_list$score==-4]#
        negTerms <- afinn_list$word[afinn_list$score==-3 | afinn_list$score==-2 | afinn_list$score==-1]#
        posTerms <- afinn_list$word[afinn_list$score==3 | afinn_list$score==2 | afinn_list$score==1]#
        vPosTerms <- afinn_list$word[afinn_list$score==5 | afinn_list$score==4]#
        ##create a matrix to store the scores and sentiment#
        final_scores <- matrix('', 0, 5)#
        ##go through each sentence in the matrix#
        for(i in text) {#
                ##pull out each word#
                wordList <- str_split(i, '\\s+')#
                words <- unlist(wordList)#
                ##match every class of word#
                vPosMatches <- match(words, vPosTerms)#
                posMatches <- match(words, posTerms)#
                vNegMatches <- match(words, vNegTerms)#
                negMatches <- match(words, negTerms)#
                #sum up number of words in each category#
                vPosMatches <- sum(!is.na(vPosMatches))#
                posMatches <- sum(!is.na(posMatches))#
                vNegMatches <- sum(!is.na(vNegMatches))#
                negMatches <- sum(!is.na(negMatches))#
                score <- c(vNegMatches, negMatches, posMatches, vPosMatches)   #
                #add row to scores table#
                newrow <- c(i, score)#
                final_scores <- rbind(final_scores, newrow)#
        }#
        ##rename the scored data and change it to a data frame#
        results <- as.data.frame(final_scores)#
        results <- cbind(results, "sentiment")#
        ##name the columns#
        colnames(results) <- c('sentence', 'vNeg', 'neg', 'pos', 'vPos', 'sentiment')#
        ##coerce the values to numerics (for some reason each value is incremented by 1, so adjusting for this as well)#
        results[, 2] <- as.numeric(results[, 2]); results[, 2] <- results[, 2] - 1#
        results[, 3] <- as.numeric(results[, 3]); results[, 3] <- results[, 3] - 1#
        results[, 4] <- as.numeric(results[, 4]); results[, 4] <- results[, 4] - 1#
        results[, 5] <- as.numeric(results[, 5]); results[, 5] <- results[, 5] - 1#
        results[, 6] <- NA ##empting out the sentiment column, to which we will be adding our answer below#
        ##counter for the while statment - we will go through each row in the matrix#
        counter <- 1#
        while(counter <= nrow(results)) {#
                ##calculate the overall sentiment of the tweet#
                sentcount <- (results[counter, 2] * -2) + (results[counter, 3] * -1) + results[counter, 4] + (results[counter, 5] * 2)#
                ##assign the overall sentiment to the sentiment column#
                results[counter, 6] <- if(sentcount < -1) {#
                        "negative"#
                } else if(sentcount > 1) {#
                        "positive"#
                } else {#
                        "neutral"#
                }#
                counter <- counter + 1#
        }
results
wordcloud(results$sentence, scale=c(6, 2), random.order = FALSE, colors=brewer.pal(8, "Paired"))
postweets <- results[results$sentiment == "positive", ]#
        neuttweets <- results[results$sentiment == "neutral", ]#
        negtweets <- results[results$sentiment == "negative", ]#
        ##get a value for the number of rows in each matrix#
        total <- dim(results)[1]#
        posnum <- dim(postweets)[1]#
        negnum <- dim(negtweets)[1]#
        neutnum <- dim(neuttweets)[1]#
        ##calculate the percentage for each sentiment value#
        posperc <- posnum/total#
        negperc <- negnum/total#
        neutperc <- neutnum/total#
        ##create the pie chart slices, labels, and add the percentage#
        slices <- c(posnum, negnum, neutnum)#
        lbls <- c("Positive", "Negative", "Nuetral")#
        pct <- round(slices/sum(slices)*100)#
        lbls <- paste(lbls, pct)#
        lbls <- paste(lbls, "%", sep = "")
pie3D(slices, labels=lbls, explode=0.1, main="Overall Sentiment of Comcast Email on Twitter")
afinn_list
